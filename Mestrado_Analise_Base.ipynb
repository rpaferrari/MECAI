{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb51cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53584614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84406802",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = rf'D:\\OneDrive\\Documentos\\Materiais didáticos\\MECAI\\Dissertação\\script\\Results'\n",
    "name_path = 'results.csv'\n",
    "complete_path = os.path.join(dir_path, name_path)\n",
    "\n",
    "df_res = pd.read_csv(complete_path, encoding='latin-1')\n",
    "df_res = df_res.loc[df_res['Valor'] != 0, :]\n",
    "df_res.loc[:, 'Tempo'] = df_res.loc[:, 'Tempo'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ed99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = rf'D:\\OneDrive\\Documentos\\Materiais didáticos\\MECAI\\Dissertação\\script\\Dados Gerados Mestrado\\Auxiliar'\n",
    "\n",
    "# df_cluster_cds_cities = pd.read_csv(os.path.join(dir, 'df_cluster_cds_cities.csv'))\n",
    "# df_cluster_pa_cities = pd.read_csv(os.path.join(dir, 'df_cluster_pa_cities.csv'))\n",
    "df_dist_CD_PA = pd.read_csv(os.path.join(dir, 'df_dist_CD_PA.csv'))\n",
    "df_dist_ZD_PA = pd.read_csv(os.path.join(dir, 'df_dist_ZD_PA.csv'))\n",
    "df_instalacao = pd.read_csv(os.path.join(dir, 'df_instalacao.csv'))\n",
    "df_desinstalacao = pd.read_csv(os.path.join(dir, 'df_desinstalacao.csv'))\n",
    "df_dist_Fab_CD = pd.read_csv(os.path.join(dir, 'df_dist_Fab_CD.csv'))\n",
    "df_dist_CD_Desc = pd.read_csv(os.path.join(dir, 'df_dist_CD_Desc.csv'))\n",
    "# df_regiao_pa_zd = pd.read_csv(os.path.join(dir, 'df_regiao_pa_zd.csv'))\n",
    "\n",
    "df_estq_inicial = pd.read_csv(os.path.join(dir, 'df_estq_inicial.csv'))\n",
    "df_estq_inicial = df_estq_inicial.loc[:, ['PA', 'lim_estq_in_pa', 'Restr_Rec', 'Restr_Env', 'custo_por_faixa', 'custo_man_por_faixa']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262824fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rpafe\\AppData\\Local\\Temp\\ipykernel_27240\\2741838635.py:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[name_col_pivot] = df[name_col_pivot].replace(mapa)\n",
      "C:\\Users\\rpafe\\AppData\\Local\\Temp\\ipykernel_27240\\2741838635.py:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[name_col_pivot] = df[name_col_pivot].replace(mapa)\n"
     ]
    }
   ],
   "source": [
    "def pivotar_demanda_mensal(df, colunas_valor, name_col, name_col_pivot, mapa=None):\n",
    "    \"\"\"\n",
    "    Função para pivotar o DataFrame de instalação, transformando as colunas de demanda mensal\n",
    "    em uma coluna única com os meses representados por números de 0 a 11.\n",
    "    \n",
    "    Parâmetros:\n",
    "    df_instalacao (DataFrame): DataFrame contendo as colunas de demanda mensal.\n",
    "    \n",
    "    Retorna:\n",
    "    DataFrame: DataFrame pivotado com colunas 'ZD', 'mes' e 'demanda'.\n",
    "    \"\"\"\n",
    " \n",
    "    # Aplicando melt\n",
    "    df = df[[name_col] + colunas_valor].melt(\n",
    "        id_vars=name_col,\n",
    "        value_vars=colunas_valor,\n",
    "        var_name= name_col_pivot,\n",
    "        value_name='demanda'\n",
    "    )\n",
    "\n",
    "    if mapa is not None:\n",
    "        df[name_col_pivot] = df[name_col_pivot].replace(mapa)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Lista com os nomes originais das colunas de demanda mensal\n",
    "colunas_demanda = [\n",
    "    'demanda_normal_Jan', 'demanda_normal_Fev', 'demanda_normal_Mar',\n",
    "    'demanda_normal_Abr', 'demanda_normal_Mai', 'demanda_normal_Jun',\n",
    "    'demanda_normal_Jul', 'demanda_normal_Ago', 'demanda_normal_Set',\n",
    "    'demanda_normal_Out', 'demanda_normal_Nov', 'demanda_normal_Dez'\n",
    "]\n",
    "\n",
    "# Dicionário para mapear os meses para valores de 1 a 12\n",
    "mapa_meses = {\n",
    "    'demanda_normal_Jan': 1, 'demanda_normal_Fev': 2, 'demanda_normal_Mar': 3,\n",
    "    'demanda_normal_Abr': 4, 'demanda_normal_Mai': 5, 'demanda_normal_Jun': 6,\n",
    "    'demanda_normal_Jul': 7, 'demanda_normal_Ago': 8, 'demanda_normal_Set': 9,\n",
    "    'demanda_normal_Out': 10, 'demanda_normal_Nov': 11, 'demanda_normal_Dez': 12\n",
    "}\n",
    "\n",
    "# Substituir nomes dos meses pelos índices numéricos\n",
    "df_inst_pivot = pivotar_demanda_mensal(df_instalacao, colunas_demanda, 'ZD', 'mes', mapa_meses)\n",
    "df_dinst_pivot = pivotar_demanda_mensal(df_desinstalacao, colunas_demanda, 'ZD', 'mes', mapa_meses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6341df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def union_dfs_with_different_columns(list_dfs):\n",
    "    \"\"\"\n",
    "    Realiza a união (concatenação) de quatro DataFrames, cada um com 3 colunas,\n",
    "    mesmo que os nomes das colunas sejam diferentes.\n",
    "\n",
    "    Args:\n",
    "        df1 (pd.DataFrame): O primeiro DataFrame.\n",
    "        df2 (pd.DataFrame): O segundo DataFrame.\n",
    "        df3 (pd.DataFrame): O terceiro DataFrame.\n",
    "        df4 (pd.DataFrame): O quarto DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Um novo DataFrame resultante da união dos quatro DataFrames,\n",
    "                      com as colunas renomeadas para 'coluna_1', 'coluna_2', 'coluna_3'.\n",
    "    \"\"\"\n",
    "    dfs = [list_dfs[i] for i in range(len(list_dfs))]\n",
    "    unified_dfs = []\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        if df.shape[1] != 3:\n",
    "            raise ValueError(f\"DataFrame {i+1} não possui 3 colunas. Possui {df.shape[1]}.\")\n",
    "\n",
    "        # Renomeia as colunas para um padrão comum\n",
    "        df_renamed = df.copy()\n",
    "        df_renamed.columns = ['Origem_Dist', 'Destino_Dist', 'Distancia']\n",
    "        unified_dfs.append(df_renamed)\n",
    "\n",
    "    # Concatena todos os DataFrames renomeados\n",
    "    union_df = pd.concat(unified_dfs, ignore_index=True)\n",
    "\n",
    "    return union_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b701aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_cd_pas = [each for each in df_dist_CD_PA.columns if each[:3] == 'PA_']\n",
    "df_dist_CD_PA_pivot = pivotar_demanda_mensal(df_dist_CD_PA, colunas_cd_pas, 'CD', 'PA')\n",
    "\n",
    "colunas_pas_zd = [each for each in df_dist_ZD_PA.columns if each[:3] == 'PA_']\n",
    "df_dist_ZD_PA_pivot = pivotar_demanda_mensal(df_dist_ZD_PA, colunas_pas_zd, 'ZD', 'PA')\n",
    "\n",
    "colunas_cd_fab = [each for each in df_dist_Fab_CD.columns if each[:3] == 'CD_']\n",
    "df_dist_Fab_CD_pivot = pivotar_demanda_mensal(df_dist_Fab_CD, colunas_cd_fab, 'FAB', 'CD')\n",
    "\n",
    "colunas_cd_desc = [each for each in df_dist_CD_Desc.columns if each[:3] == 'CD_']\n",
    "df_dist_CD_Desc_pivot = pivotar_demanda_mensal(df_dist_CD_Desc, colunas_cd_desc, 'DESC', 'CD')\n",
    "\n",
    "list_dfs = [df_dist_CD_PA_pivot, df_dist_ZD_PA_pivot, df_dist_Fab_CD_pivot, df_dist_CD_Desc_pivot]\n",
    "df_dist = union_dfs_with_different_columns(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "871b84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_join(df1, df2, left_col, right_col, prefix, how='left'):\n",
    "    # Descobrir colunas que não são chaves\n",
    "    non_key_cols = [col for col in df2.columns]\n",
    "        \n",
    "    # Fazer o merge normalmente\n",
    "    df = df1.merge(df2, left_on=left_col, right_on=right_col, how=how)\n",
    "\n",
    "    # Renomear colunas do df2 com prefixo\n",
    "    df = df.copy().rename(columns={col: f\"{prefix}{col}\" for col in non_key_cols})\n",
    "    \n",
    "    # Remover a coluna de chave do df2\n",
    "    right_list = [f\"{prefix}{right_col[i]}\" for i in range(len(right_col)) if f\"{prefix}{right_col[i]}\" in df.columns]\n",
    "    df = df.drop(columns=right_list, errors='ignore')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51e0cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = left_join(df_res, df_inst_pivot, ['Destino', 'Tempo'], ['ZD', 'mes'], 'p_inst_')\n",
    "df_res = left_join(df_res, df_dinst_pivot, ['Destino', 'Tempo'], ['ZD', 'mes'], 'p_dinst_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84668541",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = left_join(df_res, df_dist, ['Origem', 'Destino'], ['Origem_Dist', 'Destino_Dist'], 'p_')\n",
    "df_res = left_join(df_res, df_dist, ['Origem', 'Destino'], ['Destino_Dist', 'Origem_Dist'], 'p_aux_')\n",
    "df_res['p_Distancia'] = df_res['p_Distancia'].fillna(df_res['p_aux_Distancia'])\n",
    "df_res = df_res.drop(columns=['p_aux_Distancia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "868fc937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = left_join(df_res, df_estq_inicial, ['Origem'], ['PA'], 'p_ori_pa_')\n",
    "df_res = left_join(df_res, df_estq_inicial, ['Destino'], ['PA'], 'p_dest_pa_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc6b14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substituir_colunas(df, list_col1, list_col2):\n",
    "    for col1, col2 in zip(list_col1, list_col2):\n",
    "        if col1 in df.columns and col2 in df.columns:\n",
    "            df[col1] = df[col1].fillna(df[col2])\n",
    "            df = df.drop(columns=[col2])\n",
    "    return df\n",
    "\n",
    "df_res = substituir_colunas(df_res, ['p_ori_pa_custo_man_por_faixa', 'p_ori_pa_custo_por_faixa', 'p_ori_pa_lim_estq_in_pa', 'p_ori_pa_Restr_Rec', 'p_ori_pa_Restr_Env'], \\\n",
    "        ['p_dest_pa_custo_man_por_faixa', 'p_dest_pa_custo_por_faixa', 'p_dest_pa_lim_estq_in_pa', 'p_dest_pa_Restr_Rec', 'p_dest_pa_Restr_Env'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a354e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fabricantes\n",
    "rng_estq_fab = [10000, 10002]\n",
    "mrf = 9000\n",
    "maf = 9000\n",
    "ef = np.ceil(rng_estq_fab[1]*2.4)  # estf substituído por ef\n",
    "rf = round(0.2, 1)\n",
    "\n",
    "# CDs\n",
    "num_cds_sel = 1\n",
    "rng_estq_cd = [110000, 110002]\n",
    "rng_rest_cd = [rng_estq_cd[0]*0.75, rng_estq_cd[1]*0.75]\n",
    "\n",
    "ec = np.ceil(rng_estq_cd[1]*2.1)  # estc substituído por ec\n",
    "min_estq_bom_cd = np.ceil(ec*0.2)\n",
    "nc = num_cds_sel  # mc substituído por nc\n",
    "rc = round(0.7, 1)\n",
    "\n",
    "# Descartes\n",
    "dr = 1000000\n",
    "rd = round(1 - rc - rf, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae0bc846",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = 100000  # c_man_cd substituído por vi\n",
    "hi = 3000000  # c_ab_cd substituído por hi\n",
    "k = 0.087189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3337c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['p_fab_estq_fab'] = rng_estq_fab[1]\n",
    "df_res['p_fab_rest_env'] = maf\n",
    "df_res['p_fab_rest_rec'] = mrf\n",
    "df_res['p_ori_fab_lim_estq'] = ef\n",
    "\n",
    "df_res['p_cd_estq_fab'] = rng_estq_cd[1]\n",
    "df_res['p_cd_rest_env'] = rng_rest_cd[1]\n",
    "df_res['p_cd_rest_rec'] = rng_rest_cd[1]\n",
    "df_res['p_ori_cd_lim_estq'] = ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea142537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['p_custo_km'] = k\n",
    "df_res['p_cd_custo_man'] = vi\n",
    "df_res['p_cd_custo_ab'] = hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61020223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boolean_columns_generic(df, column_mappings, source_column='Abrv Var'):\n",
    "    for new_column, values in column_mappings.items():\n",
    "        df[new_column] = df[source_column].isin(values)\n",
    "    return df\n",
    "\n",
    "dados_mapeados_por_coluna = {\n",
    "    'env_cd': ['qrd', 'qap', 'qrf'], 'rec_cd': ['qac', 'qrc'], 'env_pa': ['qaj', 'qrc'], \n",
    "    'rec_pa': ['qap', 'qrp'], 'env_fab': ['qac'], 'rec_fab': ['qrf']}\n",
    "\n",
    "df_res = create_boolean_columns_generic(df_res.copy(), dados_mapeados_por_coluna)\n",
    "\n",
    "df_res['estq'] = df_res['Abrv Var'].str.startswith('s')\n",
    "df_res['movim'] = df_res['Abrv Var'].str.startswith('q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b6d7eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opção 3: Função usando pandas vetorizado (mais eficiente)\n",
    "def criar_colunas_pa_v3(df, col_origem='Origem', col_destino='Destino', \n",
    "                        prefixo='PA', nome_col1='PA', nome_col2='PA_Outros'):\n",
    "    \"\"\"\n",
    "    Versão vetorizada usando pandas - mais eficiente para DataFrames grandes.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        col_origem: nome da coluna origem\n",
    "        col_destino: nome da coluna destino\n",
    "        prefixo: string para verificar no startswith\n",
    "        nome_col1: nome da primeira coluna criada\n",
    "        nome_col2: nome da segunda coluna criada\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com as novas colunas\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Máscaras para verificar onde está o prefixo\n",
    "    mask_origem = df[col_origem].str.startswith(prefixo)\n",
    "    mask_destino = df[col_destino].str.startswith(prefixo)\n",
    "    \n",
    "    # Condição: pelo menos uma das colunas deve ter o prefixo\n",
    "    tem_prefixo = mask_origem | mask_destino\n",
    "    \n",
    "    # Primeira coluna: se origem tem prefixo, pega destino; senão pega origem\n",
    "    df[nome_col1] = np.where(tem_prefixo, \n",
    "                            np.where(mask_origem, df[col_origem], df[col_destino]),\n",
    "                            None)\n",
    "    \n",
    "    # Segunda coluna: se origem tem prefixo, pega origem; senão pega destino  \n",
    "    df[nome_col2] = np.where(tem_prefixo,\n",
    "                            np.where(mask_origem, df[col_destino], df[col_origem]),\n",
    "                            None)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_res = criar_colunas_pa_v3(df_res.copy(), col_origem='Origem', col_destino='Destino',\n",
    "                             prefixo='PA', nome_col1='PA', nome_col2='PA_Outros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58590c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opção 3: Versão em uma linha (corrigida)\n",
    "mask = ((~df_res['PA'].isna()) & (~df_res['PA_Outros'].isna())) & (df_res['rec_pa']==True)\n",
    "df_res.loc[mask, 'p_ori_pa_Restr_Rec_frac'] = (\n",
    "    df_res.loc[mask].groupby(['Tempo', 'PA'])['p_ori_pa_Restr_Rec'].transform('max') / \n",
    "    df_res.loc[mask].groupby(['Tempo', 'PA'])['PA_Outros'].transform('count')\n",
    ")\n",
    "df_res.loc[mask, 'Valor_Rec_PA'] = df_res.loc[mask, 'Valor']\n",
    "\n",
    "mask = ((~df_res['PA'].isna()) & (~df_res['PA_Outros'].isna())) & (df_res['env_pa']==True)\n",
    "df_res.loc[mask, 'p_ori_pa_Restr_Env_frac'] = (\n",
    "    df_res.loc[mask].groupby(['Tempo', 'PA'])['p_ori_pa_Restr_Env'].transform('max') / \n",
    "    df_res.loc[mask].groupby(['Tempo', 'PA'])['PA_Outros'].transform('count')\n",
    ")\n",
    "df_res.loc[mask, 'Valor_Env_PA'] = df_res.loc[mask, 'Valor']\n",
    "\n",
    "mask = (df_res['estq']==True)\n",
    "df_res.loc[mask, 'Estq_PA'] = df_res.loc[mask, 'Valor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fa24712",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['r_custo_ab_cd'] = df_res['p_cd_custo_ab'] * df_res['z_w']\n",
    "df_res['r_custo_man_cd'] = df_res['p_cd_custo_man'] * df_res['Aberto']\n",
    "df_res['r_custo_ab_pa'] = df_res['p_ori_pa_custo_por_faixa'] * df_res['z_w']\n",
    "df_res['r_custo_man_pa'] = df_res['p_ori_pa_custo_man_por_faixa'] * df_res['Aberto']\n",
    "df_res['r_custo_mov'] = df_res['Valor'] * df_res['p_Distancia'] * df_res['p_custo_km']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6c3b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res.to_csv(os.path.join(dir_path, \"df_res_v3.csv\"), encoding='latin-1', index=False)\n",
    "# df_res.to_excel(os.path.join(dir_path, \"df_res_v3.xlsx\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
